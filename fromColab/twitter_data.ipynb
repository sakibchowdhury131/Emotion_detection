{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"twitter_data.ipynb","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyM0NZ0XLx2dD6PuCpsQqisd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"QIiCXwB0mOgw","colab_type":"code","colab":{}},"source":["# -*- coding: utf-8 -*-\n","\n"," # importing libraries\n","\n","import re\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","\n","from keras.models import Sequential, load_model\n","from keras.layers import Dense, LSTM, Embedding, Dropout\n","from keras.preprocessing.text import Tokenizer \n","from keras.preprocessing.sequence import pad_sequences\n","from keras import optimizers\n","from keras import regularizers\n","\n","\n","def filter_texts (dataset): # filters texts           \n","    for i in range(0, dataset.shape[0]):\n","        comment = re.sub('[^a-zA-Z0-9\\s@]', ' ', dataset[i,1])\n","        comment = comment.lower()\n","        comment = comment.split()\n","        comment = [j for j in comment if len(j) > 1]\n","        comment = ' '.join(word for word in comment if not word.startswith('@'))\n","        dataset[i,1] = comment\n","    return dataset\n","\n","dataset1 = pd.read_csv('Tweets.csv')\n","dataset2 = pd.read_csv ('train_data.csv')\n","dataset2 = dataset2.rename (columns = {'sentiment' : 'airline_sentiment', 'content' : 'text'})\n","\n","dataset1 = dataset1.iloc[:,[1,10]]\n","\n","# analyzing the keywords\n","\n","y = dataset2.iloc[:, 0].values\n","\n","keywords = []\n","\n","for i in range (0,14640):\n","    if not(y[i] in keywords):\n","        keywords.append(y[i])\n","\n","\n","for i in range(0,30000):\n","    if (y[i]=='empty' or y[i]=='sadness' or y[i]=='worry' or y[i]=='hate' or y[i]=='anger'):\n","        y[i] = 'negative'\n","        \n","    elif y[i] =='neutral' :\n","        y[i] = 'neutral'\n","        \n","    else:\n","        y[i] = 'positive'\n","        \n","x1 = dataset1.iloc[:,:].values\n","x2 = dataset2.iloc[:,:].values\n","        \n","dataset = np.concatenate([x1,x2])\n","\n","\n","dataset = filter_texts(dataset)\n","\n","\n","# visualizing final dataset\n","\n","# dataset[:,0].value_counts().sort_index().plot.bar()\n","\n","\n","# dataset[1].str.len().plot.hist()\n","\n","#tokenizing\n","    \n","\n","tokenizer = Tokenizer(num_words=5000, split=\" \")\n","tokenizer.fit_on_texts(dataset[:,1])\n","\n","X = tokenizer.texts_to_sequences(dataset[:,1])\n","X = pad_sequences(X) # padding our text vector so they all have the same length\n","\n","\n","# Model Designing\n","\n","model = Sequential()\n","model.add(Embedding(5000, 512, input_length=X.shape[1]))\n","model.add(Dropout(0.3))\n","model.add(LSTM(300, return_sequences=True, dropout=0.3, recurrent_dropout=0.2))\n","model.add(LSTM(300, dropout=0.3, recurrent_dropout=0.2))\n","model.add(Dense(3, activation='sigmoid',kernel_regularizer=regularizers.l2(0.01),activity_regularizer=regularizers.l1(0.01)))\n","\n","\n","adam = optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n","model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n","model.summary()\n","\n","# encoding the sentiments\n","y = pd.get_dummies(dataset[:,0]).values\n","[print(dataset[i,0], y[i]) for i in range(0,15)]\n","\n","# test train split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n","\n","\n","\n","# fitting the model\n","batch_size = 30\n","epochs = 30\n","\n","history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1, shuffle = True, validation_data = (X_test,y_test))\n","\n","\n","\n","# Plot training & validation accuracy values\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('Model accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'], loc='upper left')\n","plt.show()\n","\n","# Plot training & validation loss values\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('Model loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'], loc='upper left')\n","plt.show()\n","\n","\n","model.save('19thapril220.h5')\n","\n","\n","#prediction\n","\n","predictions = model.predict(X_test)\n","[print(dataset[i,1], predictions[i], y_test[i]) for i in range(0, 5)]\n","\n","\n","#visualizing results\n","\n","pos_count, neu_count, neg_count = 0, 0, 0\n","real_pos, real_neu, real_neg = 0, 0, 0\n","for i, prediction in enumerate(predictions):\n","    if np.argmax(prediction)==2:\n","        pos_count += 1\n","    elif np.argmax(prediction)==1:\n","        neu_count += 1\n","    else:\n","        neg_count += 1\n","    \n","    if np.argmax(y_test[i])==2:\n","        real_pos += 1\n","    elif np.argmax(y_test[i])==1:    \n","        real_neu += 1\n","    else:\n","        real_neg +=1\n","\n","print('Positive predictions:', pos_count)\n","print('Neutral predictions:', neu_count)\n","print('Negative predictions:', neg_count)\n","print('Real positive:', real_pos)\n","print('Real neutral:', real_neu)\n","print('Real negative:', real_neg)\n","\n","\n","\n","# testing using self made comments\n","cmnt = [\"@sakibchowdhury : you are bullshit \", \n","        \"there is no God\",\n","        \"i'm really sad\",\n","        \"your service is good\", \n","        \"life is beautiful\",\n","        \"i really hate you sakib\",\n","        \"go to hell\",\n","        \"God sees us\",\n","        \"we should love everyone\",       \n","        \"you are a dog\"]\n","\n","for i in range(0, len(cmnt)):\n","        comment = re.sub('[^a-zA-Z0-9\\s@]', ' ', cmnt[i])\n","\n","        comment = comment.lower()\n","        comment = comment.split()\n","\n","        comment = [j for j in comment if len(j) > 1]\n","\n","        comment = ' '.join(word for word in comment if not word.startswith('@'))\n","        cmnt[i] = comment\n","\n","X_samp = tokenizer.texts_to_sequences(cmnt)\n","X_samp = pad_sequences(X_samp,31)\n","model.predict(X_samp)\n","# -*- coding: utf-8 -*-\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zodEnivowDc0","colab_type":"text"},"source":[""]}]}