{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bidirectional_lstm+sswe.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPdAsXLAvGu8HizETq10Y9e"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"Jxe9GXj_9EF7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":139},"outputId":"bfe659b9-8a23-47bc-8c4a-d092f4c93a2c","executionInfo":{"status":"ok","timestamp":1589355378781,"user_tz":-360,"elapsed":6330,"user":{"displayName":"sakib chowdhury","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMtAh0tpeZrt3Q9qj5HQ7xGfGKHp3ixCOMDofQvQ=s64","userId":"01322476370589686234"}}},"source":["pip install num2words"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting num2words\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/a2/ea800689730732e27711c41beed4b2a129b34974435bdc450377ec407738/num2words-0.5.10-py3-none-any.whl (101kB)\n","\r\u001b[K     |███▎                            | 10kB 20.8MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 20kB 3.1MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 30kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 40kB 3.0MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 51kB 3.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 61kB 4.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 71kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 81kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 92kB 6.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102kB 3.8MB/s \n","\u001b[?25hRequirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.6/dist-packages (from num2words) (0.6.2)\n","Installing collected packages: num2words\n","Successfully installed num2words-0.5.10\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1bF31J_E9mR4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"0f2ff24a-5157-439a-fd6e-279f0790e25d","executionInfo":{"status":"ok","timestamp":1589355389097,"user_tz":-360,"elapsed":2537,"user":{"displayName":"sakib chowdhury","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMtAh0tpeZrt3Q9qj5HQ7xGfGKHp3ixCOMDofQvQ=s64","userId":"01322476370589686234"}}},"source":["import nltk \n","nltk.download('punkt')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"juwByO8n9qZm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":710},"outputId":"b1350013-8feb-4cf5-9e04-918b61044888","executionInfo":{"status":"error","timestamp":1589357080777,"user_tz":-360,"elapsed":990,"user":{"displayName":"sakib chowdhury","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMtAh0tpeZrt3Q9qj5HQ7xGfGKHp3ixCOMDofQvQ=s64","userId":"01322476370589686234"}}},"source":["#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Sat May  9 22:34:44 2020\n","\n","@author: sakib\n","\n","\"\"\"\n","\n","\n","#change base directory\n","import os\n","\n","base_path=r'/content'\n","working_directory = '/content'\n","data_folder = r'/content/data'\n","embedding_folder = r'/content/embeddings'\n","models_folder = r'/content/models'\n","model_directory = working_directory+'/'+'models'\n","\n","\n","def change_base_dir(base_dir_path):\n","    \"\"\" Change the working directopry of the code\"\"\"\n","    \n","    if not os.path.exists(base_dir_path):\n","        print ('creating directory', base_dir_path)\n","        os.makedirs(base_dir_path)\n","    print ('Changing base directory to ', base_dir_path)\n","    os.chdir(base_dir_path)\n"," \n","      \n","\n","# base_folder='data'\n","# base_dir_path=base_path+'/'+base_folder\n","# change_base_dir(base_dir_path)\n","# download_data.extract_file('/media/sakib/alpha/work/EmotionDetectionDir/git/data/glove.6B.zip')\n","\n","\n","\n","change_base_dir(base_path)\n","if not os.path.exists(data_folder):\n","        os.makedirs(data_folder)\n","if not os.path.exists(embedding_folder):\n","        os.makedirs(embedding_folder)\n","if not os.path.exists(models_folder):\n","        os.makedirs(models_folder)\n","  \n","#loading necessary files\n","from load_preprocess import load_data,load_data_embedding\n","from load_preprocess import preprocess_data\n","from load_preprocess import read_labels\n","import word2vec\n","import sswe\n","import glove_file\n","import designing_network\n","import download_data\n","\n","\n","#importing libraries\n","from keras.preprocessing.text import Tokenizer \n","from keras.preprocessing.sequence import pad_sequences\n","from nltk.tokenize import word_tokenize\n","import pandas as pd\n","\n","\n","\n","# Download data\n","\n","print('Checking requirements:')\n","download_data.download_data(base_path = base_path)\n","\n","#loading data\n","print('Step1: Loading Embedding training Dataset...')\n","\n","\n","dataset_embedding = load_data_embedding(working_directory+'/data/'+'training.1600000.processed.noemoticon.csv')\n","print('Step2: Shuffling data...')\n","dataset_embedding = dataset_embedding.sample(frac=1) # reshuffling the data\n","print('Step3: Preprocessing the texts...')\n","texts = preprocess_data(dataset_embedding)\n","labels = read_labels(dataset_embedding)\n","\n","\n","# building word2vec model\n","print('Step4: Building word2vec model...')\n","EMBEDDING_DIM = 100\n","w2v = word2vec.create_word2vec(directory = working_directory+'/'+'embeddings',texts = texts ,min_count = 1,EMBEDDING_DIM = EMBEDDING_DIM)\n","\n","\n","\n","\n","# loading twitter_dataset_small\n","print('Step6: Loading Twitter Dataset')\n","dataset = load_data(working_directory+'/'+'data'+'/'+'Tweets.csv')\n","texts = preprocess_data(dataset)\n","y = pd.get_dummies(dataset['Label']).values\n","\n","\n","\n","#tokenizing\n","print ('Step7: Tokenizing...')\n","\n","tokens = []\n","for line in texts:\n","    words = word_tokenize(line)\n","    tokens.append(words)\n","\n","\n","\n","tokenizer_obj = Tokenizer()\n","tokenizer_obj.fit_on_texts(tokens)\n","sequences = tokenizer_obj.texts_to_sequences(tokens)\n","\n","\n","#padding\n","print ('Step8: Padding...')\n","tokenizer_word_index = tokenizer_obj.word_index\n","max_length = 150\n","review_pad = pad_sequences(sequences, maxlen = max_length)\n","\n","\n","\n","# train test split\n","print('Step9: train and test set generation...')\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(review_pad, y, test_size = 0.20, random_state = 0)\n","\n","num_words = len(tokenizer_word_index) + 1\n","\n","# sswe embedding matrix\n","print('Step12: Generating sswe embedding matrix...')\n","sswe_embedding_filename = working_directory+'/'+'embeddings'+'/'+'embeddings_sswe.tsv'\n","embedding_matrix_sswe = sswe.load_sswe(filename = sswe_embedding_filename, tokenizer_word_index = tokenizer_word_index, EMBEDDING_DIM = 50)\n","\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Changing base directory to  /content\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["Checking requirements:\n","Downloading stanford data\n","Extracting stanford data\n","Downloading Twitter data\n","Downloading glove model...\n","Extracting glove data\n","Step1: Loading Embedding training Dataset...\n","Step2: Shuffling data...\n","Step3: Preprocessing the texts...\n","Step4: Building word2vec model...\n","Saving word2vec model in the disk\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"},{"output_type":"stream","text":["Step6: Loading Twitter Dataset\n","Step7: Tokenizing...\n","Step8: Padding...\n","Step9: train and test set generation...\n","Step12: Generating sswe embedding matrix...\n"],"name":"stdout"},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-656cf3dd3de5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Step12: Generating sswe embedding matrix...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0msswe_embedding_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworking_directory\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'embeddings'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'embeddings_sswe.tsv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m \u001b[0membedding_matrix_sswe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msswe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_sswe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msswe_embedding_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_word_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_word_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEMBEDDING_DIM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/sswe.py\u001b[0m in \u001b[0;36mload_sswe\u001b[0;34m(filename, tokenizer_word_index, EMBEDDING_DIM)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/embeddings/embeddings_sswe.tsv'"]}]},{"cell_type":"code","metadata":{"id":"20LUMfN6HhfK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"630fc6f6-2d14-4c8b-d429-51045d4d2c5e","executionInfo":{"status":"error","timestamp":1589358849915,"user_tz":-360,"elapsed":594709,"user":{"displayName":"sakib chowdhury","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMtAh0tpeZrt3Q9qj5HQ7xGfGKHp3ixCOMDofQvQ=s64","userId":"01322476370589686234"}}},"source":["\n","\n","\n","# Download data\n","\n","print('Checking requirements:')\n","download_data.download_data(base_path = base_path)\n","\n","#loading data\n","print('Step1: Loading Embedding training Dataset...')\n","\n","\n","dataset_embedding = load_data_embedding(working_directory+'/data/'+'training.1600000.processed.noemoticon.csv')\n","print('Step2: Shuffling data...')\n","dataset_embedding = dataset_embedding.sample(frac=1) # reshuffling the data\n","print('Step3: Preprocessing the texts...')\n","texts = preprocess_data(dataset_embedding)\n","labels = read_labels(dataset_embedding)\n","\n","\n","# building word2vec model\n","print('Step4: Building word2vec model...')\n","EMBEDDING_DIM = 100\n","\n","#building sswe model\n","print('Step5: Building sswe model...')\n","sswe_model,training_word_index = sswe.sswe_model(texts, labels)\n","embedding_weights, word_indices_df, merged = sswe.save_sswe(sswe_model,training_word_index,directory = working_directory+'/'+'embeddings')\n","print('Embedding Layers are trained.')\n","\n","\n","# loading twitter_dataset_small\n","print('Step6: Loading Twitter Dataset')\n","dataset = load_data(working_directory+'/'+'data'+'/'+'Tweets.csv')\n","texts = preprocess_data(dataset)\n","y = pd.get_dummies(dataset['Label']).values\n","\n","\n","\n","#tokenizing\n","print ('Step7: Tokenizing...')\n","\n","tokens = []\n","for line in texts:\n","    words = word_tokenize(line)\n","    tokens.append(words)\n","\n","\n","\n","tokenizer_obj = Tokenizer()\n","tokenizer_obj.fit_on_texts(tokens)\n","sequences = tokenizer_obj.texts_to_sequences(tokens)\n","\n","\n","#padding\n","print ('Step8: Padding...')\n","tokenizer_word_index = tokenizer_obj.word_index\n","max_length = 150\n","review_pad = pad_sequences(sequences, maxlen = max_length)\n","\n","\n","\n","# train test split\n","print('Step9: train and test set generation...')\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(review_pad, y, test_size = 0.20, random_state = 0)\n","\n","\n","\n","\n","\n","# sswe embedding matrix\n","print('Step12: Generating sswe embedding matrix...')\n","sswe_embedding_filename = working_directory+'/'+'embeddings'+'/'+'embeddings_sswe.tsv'\n","embedding_matrix_sswe = sswe.load_sswe(filename = sswe_embedding_filename, tokenizer_word_index = tokenizer_word_index, EMBEDDING_DIM = 50)\n","\n"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Checking requirements:\n","Stanford dataset already downloaded...\n","Twitter dataset already downloaded...\n","Glove model already downloaded...\n","Step1: Loading Embedding training Dataset...\n","Step2: Shuffling data...\n","Step3: Preprocessing the texts...\n","Step4: Building word2vec model...\n","Step5: Building sswe model...\n","1600000 1600000\n","Using Keras tokenizer to tokenize and build word index\n","Size of the vocab is 352268\n","Padding sentences and shuffling the sswe train data\n","Initializing the model\n","Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (None, 15, 50)            17613500  \n","_________________________________________________________________\n","conv1d_1 (Conv1D)            (None, 11, 15)            3765      \n","_________________________________________________________________\n","max_pooling1d_1 (MaxPooling1 (None, 1, 15)             0         \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 15)                0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 15)                240       \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 2)                 32        \n","=================================================================\n","Total params: 17,617,537\n","Trainable params: 17,617,537\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"},{"output_type":"stream","text":["/content/sswe.py:129: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n","  model.fit(train_x, train_y,nb_epoch=no_epochs, batch_size=batch_size,validation_data=(valid_x, valid_y),callbacks=[mcp])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train on 1280000 samples, validate on 320000 samples\n","Epoch 1/5\n","1280000/1280000 [==============================] - 69s 54us/step - loss: 0.5384 - acc: 0.7412 - val_loss: 0.5069 - val_acc: 0.7598\n","Epoch 2/5\n","1280000/1280000 [==============================] - 69s 54us/step - loss: 0.5133 - acc: 0.7513 - val_loss: 0.5293 - val_acc: 0.7477\n","Epoch 3/5\n","1280000/1280000 [==============================] - 69s 54us/step - loss: 0.5291 - acc: 0.7452 - val_loss: 0.5341 - val_acc: 0.7426\n","Epoch 4/5\n","1280000/1280000 [==============================] - 69s 54us/step - loss: 0.5436 - acc: 0.7368 - val_loss: 0.5688 - val_acc: 0.7316\n","Epoch 5/5\n","1280000/1280000 [==============================] - 68s 53us/step - loss: 0.5645 - acc: 0.7217 - val_loss: 0.5629 - val_acc: 0.7304\n","(352268, 2) (352270, 51)\n","(352268, 52)\n","Embedding Layers are trained.\n","Step6: Loading Twitter Dataset\n","Step7: Tokenizing...\n","Step8: Padding...\n","Step9: train and test set generation...\n","Step12: Generating sswe embedding matrix...\n","Step13: designing lstm+sswe model...\n"],"name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-88e8ac69c628>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;31m# training the sswe model with lstm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Step13: designing lstm+sswe model...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m \u001b[0msswe_lstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdesigning_networks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_architecture_sswe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_matrix_sswe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mEMBEDDING_DIM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0msswe_lstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdesigning_networks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msswe_lstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0mdesigning_networks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_network_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msswe_lstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'sswe_lstm'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdirectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/designing_networks.py\u001b[0m in \u001b[0;36mmodel_architecture_sswe\u001b[0;34m(embedding_matrix, num_words, EMBEDDING_DIM, max_length)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mmodel_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0membedding_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurrent_dropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0mmodel_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}]},{"cell_type":"code","metadata":{"id":"5JlgH80XL-wq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":666},"outputId":"a30f1697-6c31-4205-dda8-7b4c3db50ecd","executionInfo":{"status":"ok","timestamp":1589359548405,"user_tz":-360,"elapsed":401451,"user":{"displayName":"sakib chowdhury","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMtAh0tpeZrt3Q9qj5HQ7xGfGKHp3ixCOMDofQvQ=s64","userId":"01322476370589686234"}}},"source":["\n","\n","import designing_networkss\n","# training the sswe model with lstm\n","print('Step13: designing lstm+sswe model...')\n","sswe_lstm = designing_networkss.model_architecture_sswe(embedding_matrix_sswe, num_words,EMBEDDING_DIM = 50 , max_length = max_length)\n","sswe_lstm, history = designing_networkss.fit_network(sswe_lstm, X_train, X_test, y_train, y_test)\n","designing_networkss.save_network_model(sswe_lstm, modelname = 'sswe_lstm',directory = model_directory)\n","# loaded_model = designing_network.load_network_model( directory = working_directory+'/'+'models', jsonfile = 'sswe_lstm.json', h5file = 'sswe_lstm.h5')\n","# score = designing_network.analyze_performance(loaded_model, X_test, y_test)\n","# print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Step13: designing lstm+sswe model...\n","Model: \"sequential_3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_2 (Embedding)      (None, 150, 50)           691150    \n","_________________________________________________________________\n","bidirectional_1 (Bidirection (None, 64)                21248     \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 3)                 195       \n","=================================================================\n","Total params: 712,593\n","Trainable params: 21,443\n","Non-trainable params: 691,150\n","_________________________________________________________________\n","Train on 11712 samples, validate on 2928 samples\n","Epoch 1/10\n","11712/11712 [==============================] - 40s 3ms/step - loss: 0.4876 - accuracy: 0.7803 - val_loss: 0.4350 - val_accuracy: 0.8067\n","Epoch 2/10\n","11712/11712 [==============================] - 40s 3ms/step - loss: 0.4309 - accuracy: 0.8092 - val_loss: 0.4053 - val_accuracy: 0.8181\n","Epoch 3/10\n","11712/11712 [==============================] - 40s 3ms/step - loss: 0.4100 - accuracy: 0.8193 - val_loss: 0.3956 - val_accuracy: 0.8258\n","Epoch 4/10\n","11712/11712 [==============================] - 40s 3ms/step - loss: 0.3994 - accuracy: 0.8244 - val_loss: 0.3881 - val_accuracy: 0.8278\n","Epoch 5/10\n","11712/11712 [==============================] - 40s 3ms/step - loss: 0.3919 - accuracy: 0.8287 - val_loss: 0.3794 - val_accuracy: 0.8325\n","Epoch 6/10\n","11712/11712 [==============================] - 40s 3ms/step - loss: 0.3845 - accuracy: 0.8311 - val_loss: 0.3779 - val_accuracy: 0.8336\n","Epoch 7/10\n","11712/11712 [==============================] - 40s 3ms/step - loss: 0.3818 - accuracy: 0.8319 - val_loss: 0.3774 - val_accuracy: 0.8327\n","Epoch 8/10\n","11712/11712 [==============================] - 39s 3ms/step - loss: 0.3768 - accuracy: 0.8338 - val_loss: 0.3765 - val_accuracy: 0.8337\n","Epoch 9/10\n","11712/11712 [==============================] - 40s 3ms/step - loss: 0.3740 - accuracy: 0.8358 - val_loss: 0.3752 - val_accuracy: 0.8350\n","Epoch 10/10\n","11712/11712 [==============================] - 40s 3ms/step - loss: 0.3704 - accuracy: 0.8375 - val_loss: 0.3744 - val_accuracy: 0.8378\n","Saved model to disk\n"],"name":"stdout"}]}]}