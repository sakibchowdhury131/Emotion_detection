{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"word2vec+gru.ipynb","provenance":[],"authorship_tag":"ABX9TyOi2h2Ob2AlLoIgb5h02FPK"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"xE8xprvG5K4n","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"outputId":"b62b59c1-1c1f-4b17-8f35-41b015c0a99b","executionInfo":{"status":"ok","timestamp":1589354930254,"user_tz":-360,"elapsed":6049,"user":{"displayName":"sakib chowdhury","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMtAh0tpeZrt3Q9qj5HQ7xGfGKHp3ixCOMDofQvQ=s64","userId":"01322476370589686234"}}},"source":["pip install num2words\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting num2words\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/a2/ea800689730732e27711c41beed4b2a129b34974435bdc450377ec407738/num2words-0.5.10-py3-none-any.whl (101kB)\n","\r\u001b[K     |███▎                            | 10kB 16.4MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 20kB 3.1MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 30kB 3.7MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 40kB 4.1MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 51kB 3.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 61kB 4.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 71kB 4.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 81kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 92kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102kB 3.7MB/s \n","\u001b[?25hRequirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.6/dist-packages (from num2words) (0.6.2)\n","Installing collected packages: num2words\n","Successfully installed num2words-0.5.10\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oadEY0K7708a","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"c0b6a0f2-ee24-482d-a74c-2cec51cf8ee5","executionInfo":{"status":"ok","timestamp":1589354933282,"user_tz":-360,"elapsed":2409,"user":{"displayName":"sakib chowdhury","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMtAh0tpeZrt3Q9qj5HQ7xGfGKHp3ixCOMDofQvQ=s64","userId":"01322476370589686234"}}},"source":["import nltk\n","nltk.download('punkt')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"atb3fGqF76m_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"20ba0507-2591-42fb-cbf4-ebdbfb3728e7","executionInfo":{"status":"error","timestamp":1589356691199,"user_tz":-360,"elapsed":1709021,"user":{"displayName":"sakib chowdhury","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMtAh0tpeZrt3Q9qj5HQ7xGfGKHp3ixCOMDofQvQ=s64","userId":"01322476370589686234"}}},"source":["#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Sat May  9 22:34:44 2020\n","\n","@author: sakib\n","\n","\"\"\"\n","\n","\n","#change base directory\n","import os\n","\n","base_path=r'/content'\n","working_directory = '/content'\n","data_folder = r'/content/data'\n","embedding_folder = r'/content/embeddings'\n","models_folder = r'/content/models'\n","model_directory = working_directory+'/'+'models'\n","\n","\n","def change_base_dir(base_dir_path):\n","    \"\"\" Change the working directopry of the code\"\"\"\n","    \n","    if not os.path.exists(base_dir_path):\n","        print ('creating directory', base_dir_path)\n","        os.makedirs(base_dir_path)\n","    print ('Changing base directory to ', base_dir_path)\n","    os.chdir(base_dir_path)\n"," \n","      \n","\n","# base_folder='data'\n","# base_dir_path=base_path+'/'+base_folder\n","# change_base_dir(base_dir_path)\n","# download_data.extract_file('/media/sakib/alpha/work/EmotionDetectionDir/git/data/glove.6B.zip')\n","\n","\n","\n","change_base_dir(base_path)\n","if not os.path.exists(data_folder):\n","        os.makedirs(data_folder)\n","if not os.path.exists(embedding_folder):\n","        os.makedirs(embedding_folder)\n","if not os.path.exists(models_folder):\n","        os.makedirs(models_folder)\n","  \n","#loading necessary files\n","from load_preprocess import load_data,load_data_embedding\n","from load_preprocess import preprocess_data\n","from load_preprocess import read_labels\n","import word2vec\n","import sswe\n","import glove_file\n","import designing_network\n","import download_data\n","\n","\n","#importing libraries\n","from keras.preprocessing.text import Tokenizer \n","from keras.preprocessing.sequence import pad_sequences\n","from nltk.tokenize import word_tokenize\n","import pandas as pd\n","\n","\n","\n","# Download data\n","\n","print('Checking requirements:')\n","download_data.download_data(base_path = base_path)\n","\n","#loading data\n","print('Step1: Loading Embedding training Dataset...')\n","\n","\n","dataset_embedding = load_data_embedding(working_directory+'/data/'+'training.1600000.processed.noemoticon.csv')\n","print('Step2: Shuffling data...')\n","dataset_embedding = dataset_embedding.sample(frac=1) # reshuffling the data\n","print('Step3: Preprocessing the texts...')\n","texts = preprocess_data(dataset_embedding)\n","labels = read_labels(dataset_embedding)\n","\n","\n","# building word2vec model\n","print('Step4: Building word2vec model...')\n","EMBEDDING_DIM = 100\n","w2v = word2vec.create_word2vec(directory = working_directory+'/'+'embeddings',texts = texts ,min_count = 1,EMBEDDING_DIM = EMBEDDING_DIM)\n","\n","\n","\n","#building sswe model\n","print('Step5: Building sswe model...')\n","sswe_model,training_word_index = sswe.sswe_model(texts, labels)\n","embedding_weights, word_indices_df, merged = sswe.save_sswe(sswe_model,training_word_index,directory = working_directory+'/'+'embeddings')\n","print('Embedding Layers are trained.')\n","\n","\n","\n","# loading twitter_dataset_small\n","print('Step6: Loading Twitter Dataset')\n","dataset = load_data(working_directory+'/'+'data'+'/'+'Tweets.csv')\n","texts = preprocess_data(dataset)\n","y = pd.get_dummies(dataset['Label']).values\n","\n","\n","\n","#tokenizing\n","print ('Step7: Tokenizing...')\n","\n","tokens = []\n","for line in texts:\n","    words = word_tokenize(line)\n","    tokens.append(words)\n","\n","\n","\n","tokenizer_obj = Tokenizer()\n","tokenizer_obj.fit_on_texts(tokens)\n","sequences = tokenizer_obj.texts_to_sequences(tokens)\n","\n","\n","#padding\n","print ('Step8: Padding...')\n","tokenizer_word_index = tokenizer_obj.word_index\n","max_length = 150\n","review_pad = pad_sequences(sequences, maxlen = max_length)\n","\n","\n","\n","# train test split\n","print('Step9: train and test set generation...')\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(review_pad, y, test_size = 0.20, random_state = 0)\n","\n","\n","# word2vec Embedding Matrix\n","print('Step10: Generating word2vec embedding matrix...')\n","num_words = len(tokenizer_word_index) + 1\n","embedding_matrix_w2v = word2vec.load_word2vec(working_directory+'/'+'embeddings'+'/'+'embeddings_w2v.txt', tokenizer_word_index=tokenizer_word_index, EMBEDDING_DIM=EMBEDDING_DIM)\n","\n","\n","# training the word2vec model with lstm\n","print('Step11: designing lstm+w2v model...')\n","\n","w2v_lstm = designing_network.model_architecture_word2vec(embedding_matrix_w2v, num_words,EMBEDDING_DIM = EMBEDDING_DIM , max_length = max_length)\n","w2v_lstm, history = designing_network.fit_network(w2v_lstm, X_train, X_test, y_train, y_test)\n","designing_network.save_network_model(w2v_lstm, modelname = 'w2v_lstm',directory = model_directory)\n","# loaded_model = designing_network.load_network_model( directory = working_directory+'/'+'models', jsonfile = 'w2v_lstm.json', h5file = 'w2v_lstm.h5')\n","# score = designing_network.analyze_performance(loaded_model, X_test, y_test)\n","# print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n","\n","# sswe embedding matrix\n","print('Step12: Generating sswe embedding matrix...')\n","sswe_embedding_filename = working_directory+'/'+'embeddings'+'/'+'embeddings_sswe.tsv'\n","embedding_matrix_sswe = sswe.load_sswe(filename = sswe_embedding_filename, tokenizer_word_index = tokenizer_word_index, EMBEDDING_DIM = 50)\n","\n","\n","# training the sswe model with lstm\n","print('Step13: designing lstm+sswe model...')\n","sswe_lstm = designing_network.model_architecture_sswe(embedding_matrix_sswe, num_words,EMBEDDING_DIM = 50 , max_length = max_length)\n","sswe_lstm, history = designing_network.fit_network(sswe_lstm, X_train, X_test, y_train, y_test)\n","designing_network.save_network_model(sswe_lstm, modelname = 'sswe_lstm',directory = model_directory)\n","# loaded_model = designing_network.load_network_model( directory = working_directory+'/'+'models', jsonfile = 'sswe_lstm.json', h5file = 'sswe_lstm.h5')\n","# score = designing_network.analyze_performance(loaded_model, X_test, y_test)\n","# print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n","\n","# glove embedding matrix\n","print ('Step 14: Generating glove embedding matrix...')\n","embedding_matrix_glove = glove_file.load_glove(working_directory+'/'+'data'+'/'+'glove.6B.100d.txt', tokenizer_word_index=tokenizer_word_index, EMBEDDING_DIM=EMBEDDING_DIM)\n","\n","\n","\n","# training the glove model with lstm\n","print('Step15: designing lstm+w2v model...')\n","glove_lstm = designing_network.model_architecture_glove(embedding_matrix_glove, num_words,EMBEDDING_DIM = EMBEDDING_DIM , max_length = max_length)\n","glove_lstm, history = designing_network.fit_network(glove_lstm, X_train, X_test, y_train, y_test)\n","designing_network.save_network_model(glove_lstm, modelname = 'glove_lstm',directory = model_directory)\n","# loaded_model = designing_network.load_network_model( directory = working_directory+'/'+'models', jsonfile = 'glove_lstm.json', h5file = 'glove_lstm.h5')\n","# score = designing_network.analyze_performance(loaded_model, X_test, y_test)\n","# print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n","\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Changing base directory to  /content\n","Checking requirements:\n","Downloading stanford data\n","Extracting stanford data\n","Downloading Twitter data\n","Downloading glove model...\n","Extracting glove data\n","Step1: Loading Embedding training Dataset...\n","Step2: Shuffling data...\n","Step3: Preprocessing the texts...\n","Step4: Building word2vec model...\n","Saving word2vec model in the disk\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"},{"output_type":"stream","text":["Step5: Building sswe model...\n","1600000 1600000\n","Using Keras tokenizer to tokenize and build word index\n","Size of the vocab is 352268\n","Padding sentences and shuffling the sswe train data\n","Initializing the model\n","Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (None, 15, 50)            17613500  \n","_________________________________________________________________\n","conv1d_1 (Conv1D)            (None, 11, 15)            3765      \n","_________________________________________________________________\n","max_pooling1d_1 (MaxPooling1 (None, 1, 15)             0         \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 15)                0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 15)                240       \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 2)                 32        \n","=================================================================\n","Total params: 17,617,537\n","Trainable params: 17,617,537\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"},{"output_type":"stream","text":["/content/sswe.py:129: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n","  model.fit(train_x, train_y,nb_epoch=no_epochs, batch_size=batch_size,validation_data=(valid_x, valid_y),callbacks=[mcp])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train on 1280000 samples, validate on 320000 samples\n","Epoch 1/5\n","1280000/1280000 [==============================] - 67s 53us/step - loss: 0.5382 - acc: 0.7393 - val_loss: 0.5118 - val_acc: 0.7583\n","Epoch 2/5\n","1280000/1280000 [==============================] - 68s 53us/step - loss: 0.5070 - acc: 0.7536 - val_loss: 0.5164 - val_acc: 0.7485\n","Epoch 3/5\n","1280000/1280000 [==============================] - 68s 53us/step - loss: 0.5289 - acc: 0.7460 - val_loss: 0.5343 - val_acc: 0.7429\n","Epoch 4/5\n","1280000/1280000 [==============================] - 68s 53us/step - loss: 0.5498 - acc: 0.7248 - val_loss: 0.5452 - val_acc: 0.7261\n","Epoch 5/5\n","1280000/1280000 [==============================] - 67s 53us/step - loss: 0.5682 - acc: 0.7144 - val_loss: 0.5603 - val_acc: 0.7111\n","(352268, 2) (352270, 51)\n","(352268, 52)\n","Embedding Layers are trained.\n","Step6: Loading Twitter Dataset\n","Step7: Tokenizing...\n","Step8: Padding...\n","Step9: train and test set generation...\n","Step10: Generating word2vec embedding matrix...\n","Step11: designing lstm+w2v model...\n"],"name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-87b2039549e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Step11: designing lstm+w2v model...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m \u001b[0mw2v_lstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdesigning_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_architecture_word2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_matrix_w2v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mEMBEDDING_DIM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEMBEDDING_DIM\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0mw2v_lstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdesigning_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v_lstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0mdesigning_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_network_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v_lstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'w2v_lstm'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdirectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/designing_network.py\u001b[0m in \u001b[0;36mmodel_architecture_word2vec\u001b[0;34m(embedding_matrix, num_words, EMBEDDING_DIM, max_length)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mmodel_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0membedding_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mmodel_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGRU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurrent_dropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mmodel_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'GRU' is not defined"]}]},{"cell_type":"code","metadata":{"id":"GGmr5oYwC4iX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":646},"outputId":"20aa2e20-a0f0-4192-cb62-c52f35f9ed56","executionInfo":{"status":"ok","timestamp":1589357176061,"user_tz":-360,"elapsed":3008,"user":{"displayName":"sakib chowdhury","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMtAh0tpeZrt3Q9qj5HQ7xGfGKHp3ixCOMDofQvQ=s64","userId":"01322476370589686234"}}},"source":["\n","import designing_networks\n","# training the word2vec model with lstm\n","print('Step11: designing lstm+w2v model...')\n","\n","w2v_lstm = designing_networks.model_architecture_word2vec(embedding_matrix_w2v, num_words,EMBEDDING_DIM = EMBEDDING_DIM , max_length = max_length)\n","w2v_lstm, history = designing_networks.fit_network(w2v_lstm, X_train, X_test, y_train, y_test)\n","designing_networks.save_network_model(w2v_lstm, modelname = 'w2v_lstm',directory = model_directory)\n","\n"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Step11: designing lstm+w2v model...\n","Model: \"sequential_4\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_3 (Embedding)      (None, 150, 100)          1382300   \n","_________________________________________________________________\n","gru_1 (GRU)                  (None, 32)                12768     \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 3)                 99        \n","=================================================================\n","Total params: 1,395,167\n","Trainable params: 12,867\n","Non-trainable params: 1,382,300\n","_________________________________________________________________\n","Train on 11712 samples, validate on 2928 samples\n","Epoch 1/10\n","11712/11712 [==============================] - 30s 3ms/step - loss: 0.4835 - accuracy: 0.7832 - val_loss: 0.5203 - val_accuracy: 0.7766\n","Epoch 2/10\n","11712/11712 [==============================] - 31s 3ms/step - loss: 0.4385 - accuracy: 0.8073 - val_loss: 0.4915 - val_accuracy: 0.7887\n","Epoch 3/10\n","11712/11712 [==============================] - 30s 3ms/step - loss: 0.4145 - accuracy: 0.8176 - val_loss: 0.4782 - val_accuracy: 0.7877\n","Epoch 4/10\n","11712/11712 [==============================] - 30s 3ms/step - loss: 0.4004 - accuracy: 0.8258 - val_loss: 0.4657 - val_accuracy: 0.7913\n","Epoch 5/10\n","11712/11712 [==============================] - 30s 3ms/step - loss: 0.3883 - accuracy: 0.8330 - val_loss: 0.4685 - val_accuracy: 0.7949\n","Epoch 6/10\n","11712/11712 [==============================] - 29s 3ms/step - loss: 0.3856 - accuracy: 0.8324 - val_loss: 0.4623 - val_accuracy: 0.7923\n","Epoch 7/10\n","11712/11712 [==============================] - 29s 2ms/step - loss: 0.3680 - accuracy: 0.8410 - val_loss: 0.4522 - val_accuracy: 0.7951\n","Epoch 8/10\n","11712/11712 [==============================] - 31s 3ms/step - loss: 0.3630 - accuracy: 0.8427 - val_loss: 0.4560 - val_accuracy: 0.7970\n","Epoch 9/10\n","11712/11712 [==============================] - 31s 3ms/step - loss: 0.3594 - accuracy: 0.8432 - val_loss: 0.4501 - val_accuracy: 0.7979\n","Epoch 10/10\n","11712/11712 [==============================] - 29s 3ms/step - loss: 0.3504 - accuracy: 0.8482 - val_loss: 0.4392 - val_accuracy: 0.8041\n","Saved model to disk\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FvEpr1owEoHj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"bc4a57ac-c0b9-40ef-8e59-63517353c707","executionInfo":{"status":"ok","timestamp":1589357270195,"user_tz":-360,"elapsed":2900,"user":{"displayName":"sakib chowdhury","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMtAh0tpeZrt3Q9qj5HQ7xGfGKHp3ixCOMDofQvQ=s64","userId":"01322476370589686234"}}},"source":["loaded_model = designing_networks.load_network_model( directory = working_directory+'/'+'models', jsonfile = 'w2v_lstm.json', h5file = 'w2v_lstm.h5')\n","score = designing_networks.analyze_performance(loaded_model, X_test, y_test)\n","print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Loaded model from disk\n","2928/2928 [==============================] - 1s 466us/step\n","accuracy: 80.41%\n"],"name":"stdout"}]}]}