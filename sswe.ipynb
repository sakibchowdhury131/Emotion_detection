{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sswe.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPbhPrpUZ6cVk6kc8gXVdUd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sakibchowdhury131/Emotion_detection/blob/master/sswe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ye90BBP4CMk5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "e503c95d-fb0a-4ad7-ec8f-9f76d6ecbb54"
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Thu May  7 22:49:58 2020\n",
        "\n",
        "@author: sakib\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from zipfile import ZipFile\n",
        "import urllib.request\n",
        "from tempfile import mktemp\n",
        "\n",
        "base_path=r'/content'\n",
        "base_folder='data'\n",
        "\n",
        "# URL to download the sentiment140 dataset\n",
        "data_url='http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip'\n",
        "\n",
        "def change_base_dir(base_dir_path):\n",
        "    \"\"\" Change the working directopry of the code\"\"\"\n",
        "    \n",
        "    if not os.path.exists(base_dir_path):\n",
        "        print ('creating directory', base_dir_path)\n",
        "        os.makedirs(base_dir_path)\n",
        "    print ('Changing base directory to ', base_dir_path)\n",
        "    os.chdir(base_dir_path)\n",
        "\n",
        "def download_data(download_url, filename='downloaded_data.zip'):\n",
        "    \"\"\" Download and extract data \"\"\"\n",
        "    \n",
        "    downloaded_filename = os.path.join('.', filename)\n",
        "    print ('Step 1: Downloading data')\n",
        "    urllib.request.urlretrieve(download_url,downloaded_filename)\n",
        "    print ('Step 2: Extracting data')\n",
        "    zipfile=ZipFile(downloaded_filename)\n",
        "    zipfile.extractall('./')\n",
        "    zipfile.close()\n",
        "\n",
        "def extract_tweets_and_labels(filename ):\n",
        "    \"\"\" Extract tweets and labels from the downloaded data\"\"\"\n",
        "    \n",
        "    print ('Step 3: Reading the data as a dataframe')\n",
        "    df=pd.read_csv(filename, header=None, encoding='iso-8859-1')    \n",
        "    df.columns=['Label','TweetId','Date','Query','User','Text']\n",
        "    print ('Read {} lines'.format(df.shape[0]))\n",
        "    print ('Discarding neutral tweets')\n",
        "    df=df[df.Label!=2]\n",
        "    print ('No of lines in the data after filtering neutral tweets: {}'.format(df.shape[0]))\n",
        "    print ('Step 4: Shuffling the data')\n",
        "    train_length=int(df.shape[0]*0.8)    \n",
        "    df=df.sample(frac=1) # reshuffling the data\n",
        "      \n",
        "    df['Text']=df['Text'].astype(str).apply(lambda x:x.strip())#.encode('ascii','ignore')#str.decode('utf8','ignore')#.str.encode('ascii','ignore')\n",
        "    print (df.head())\n",
        "    print ('Step 5: Dividing into test and train datasets')\n",
        "    df_train = df.iloc[:train_length, :]\n",
        "    df_test = df.iloc[train_length:, :]    \n",
        "    \n",
        "    print ('Step 6: Exporting the train and test datasets')    \n",
        "    print ('Exporting training data of rows {}'.format(df_train.shape[0]))\n",
        "    export_prefix='training'\n",
        "    df_train[['Label']].to_csv(export_prefix+'_label.csv', header=False, index=False)\n",
        "    df_train[['Text']].to_csv(export_prefix+'_text.csv', header=False, index=False)\n",
        "    print ('Target distribution in the training data is as follows')\n",
        "    print ('\\n',df_train['Label'].value_counts()) \n",
        "    \n",
        "    print ('Exporting training data of rows {}'.format(df_test.shape[0]))\n",
        "    export_prefix='testing'\n",
        "    df_test[['Label']].to_csv(export_prefix+'_label.csv', header=False, index=False)\n",
        "    df_test[['Text']].to_csv(export_prefix+'_text.csv', header=False, index=False)\n",
        "    print ('Target distribution in the testing data is as follows')\n",
        "    print ('\\n',df_test['Label'].value_counts())\n",
        "\n",
        "\n",
        "base_dir_path=base_path+'/'+base_folder\n",
        "change_base_dir(base_dir_path)\n",
        "download_data(data_url)\n",
        "extract_tweets_and_labels('training.1600000.processed.noemoticon.csv')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "creating directory /content/data\n",
            "Changing base directory to  /content/data\n",
            "Step 1: Downloading data\n",
            "Step 2: Extracting data\n",
            "Step 3: Reading the data as a dataframe\n",
            "Read 1600000 lines\n",
            "Discarding neutral tweets\n",
            "No of lines in the data after filtering neutral tweets: 1600000\n",
            "Step 4: Shuffling the data\n",
            "         Label  ...                                               Text\n",
            "1502276      4  ...  Andy is adorable! I'm still trying to upload p...\n",
            "942199       4  ...  @dribblebuster spoke to the shop in teddington...\n",
            "1099884      4  ...  @Fiona_of_Toorak Bit of a love-hate relationsh...\n",
            "167952       0  ...  Had a fantastic day with an amazing girl! Just...\n",
            "1102205      4  ...  @docbaty B/W prepping to be a daddy, sitting o...\n",
            "\n",
            "[5 rows x 6 columns]\n",
            "Step 5: Dividing into test and train datasets\n",
            "Step 6: Exporting the train and test datasets\n",
            "Exporting training data of rows 1280000\n",
            "Target distribution in the training data is as follows\n",
            "\n",
            " 4    640198\n",
            "0    639802\n",
            "Name: Label, dtype: int64\n",
            "Exporting training data of rows 320000\n",
            "Target distribution in the testing data is as follows\n",
            "\n",
            " 0    160198\n",
            "4    159802\n",
            "Name: Label, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVUMinYVCzcB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "import os\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKvtv2XFC6Hl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "a93cae4b-6bc7-414d-8953-c5a981d1af7f"
      },
      "source": [
        "pip install num2words\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting num2words\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/a2/ea800689730732e27711c41beed4b2a129b34974435bdc450377ec407738/num2words-0.5.10-py3-none-any.whl (101kB)\n",
            "\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–                            | 10kB 17.9MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                         | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                      | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                   | 40kB 3.4MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 71kB 2.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 81kB 3.0MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 92kB 2.5MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102kB 2.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.6/dist-packages (from num2words) (0.6.2)\n",
            "Installing collected packages: num2words\n",
            "Successfully installed num2words-0.5.10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KORR1gbSCh5h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "outputId": "d111563e-78e2-4bc1-c45d-ece22d06488b"
      },
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from timeit import default_timer as timer\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Input, Dense, Flatten, Embedding\n",
        "from keras.layers.pooling import GlobalMaxPooling1D,MaxPooling1D\n",
        "from keras.layers.convolutional import Convolution1D\n",
        "from keras.layers.core import Lambda\n",
        "from keras import optimizers\n",
        "from keras.models import Model\n",
        "from keras.regularizers import l1\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn import svm, metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from IPython.display import SVG\n",
        "import pydot\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "import re\n",
        "import io\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import num2words\n",
        "\n",
        "random_seed=1\n",
        "np.random.seed(random_seed)\n",
        "\n",
        "\n",
        "\n",
        "data_dir = r'/content/data'\n",
        "embedding_folder = 'vectors'\n",
        "model_identifier = 'SSWE_Basic_Keras_w_CNTK'\n",
        "\n",
        "if not os.path.exists(embedding_folder):\n",
        "    os.makedirs(embedding_folder)\n",
        "\n",
        "max_sequence_length = 15 # each sentence of the input should be padded to have at least this many tokens\n",
        "embedding_dim \t\t= 50 # Embedding layer size\n",
        "no_filters\t\t\t= 15 # No of filters for the convolution layer\n",
        "filter_size\t\t\t= 5  # Filter size for the convolution layer\n",
        "trainable \t\t\t= True # flag specifying whether the embedding layer weights should be changed during the training or not\n",
        "batch_size \t\t\t= 1024*6 # batch size can be increased to have better gpu utilization\n",
        "#batch_size \t\t\t= 64 # batch size can be increased to have better gpu utilization\n",
        "no_epochs \t\t\t= 5 # No of training epochs\n",
        "\n",
        "\n",
        "\n",
        "pos_emoticons=[\"(^.^)\",\"(^-^)\",\"(^_^)\",\"(^_~)\",\"(^3^)\",\"(^o^)\",\"(~_^)\",\"*)\",\":)\",\":*\",\":-*\",\":]\",\":^)\",\":}\",\n",
        "               \":>\",\":3\",\":b\",\":-b\",\":c)\",\":D\",\":-D\",\":O\",\":-O\",\":o)\",\":p\",\":-p\",\":P\",\":-P\",\":Ã\",\":-Ã\",\":X\",\n",
        "               \":-X\",\";)\",\";-)\",\";]\",\";D\",\"^)\",\"^.~\",\"_)m\",\" ~.^\",\"<=8\",\"<3\",\"<333\",\"=)\",\"=///=\",\"=]\",\"=^_^=\",\n",
        "               \"=<_<=\",\"=>.<=\",\" =>.>=\",\" =3\",\"=D\",\"=p\",\"0-0\",\"0w0\",\"8D\",\"8O\",\"B)\",\"C:\",\"d'-'\",\"d(>w<)b\",\":-)\",\n",
        "               \"d^_^b\",\"qB-)\",\"X3\",\"xD\",\"XD\",\"XP\",\"Ê˜â€¿Ê˜\",\"â¤\",\"ğŸ’œ\",\"ğŸ’š\",\"ğŸ’•\",\"ğŸ’™\",\"ğŸ’›\",\"ğŸ’“\",\"ğŸ’\",\"ğŸ’–\",\"ğŸ’\",\n",
        "               \"ğŸ’˜\",\"ğŸ’—\",\"ğŸ˜—\",\"ğŸ˜˜\",\"ğŸ˜™\",\"ğŸ˜š\",\"ğŸ˜»\",\"ğŸ˜€\",\"ğŸ˜\",\"ğŸ˜ƒ\",\"â˜º\",\"ğŸ˜„\",\"ğŸ˜†\",\"ğŸ˜‡\",\"ğŸ˜‰\",\"ğŸ˜Š\",\"ğŸ˜‹\",\"ğŸ˜\",\n",
        "               \"ğŸ˜\",\"ğŸ˜\",\"ğŸ˜›\",\"ğŸ˜œ\",\"ğŸ˜\",\"ğŸ˜®\",\"ğŸ˜¸\",\"ğŸ˜¹\",\"ğŸ˜º\",\"ğŸ˜»\",\"ğŸ˜¼\",\"ğŸ‘\"]\n",
        "\n",
        "neg_emoticons=[\"--!--\",\"(,_,)\",\"(-.-)\",\"(._.)\",\"(;.;)9\",\"(>.<)\",\"(>_<)\",\"(>_>)\",\"(Â¬_Â¬)\",\"(X_X)\",\":&\",\":(\",\":'(\",\n",
        "               \":-(\",\":-/\",\":-@[1]\",\":[\",\":\\\\\",\":{\",\":<\",\":-9\",\":c\",\":S\",\";(\",\";*(\",\";_;\",\"^>_>^\",\"^o)\",\"_|_\",\n",
        "               \"`_Â´\",\"</3\",\"<=3\",\"=/\",\"=\\\\\",\">:(\",\">:-(\",\"ğŸ’”\",\"â˜¹ï¸\",\"ğŸ˜Œ\",\"ğŸ˜’\",\"ğŸ˜“\",\"ğŸ˜”\",\"ğŸ˜•\",\"ğŸ˜–\",\"ğŸ˜\",\"ğŸ˜Ÿ\",\n",
        "               \"ğŸ˜ \",\"ğŸ˜¡\",\"ğŸ˜¢\",\"ğŸ˜£\",\"ğŸ˜¤\",\"ğŸ˜¥\",\"ğŸ˜¦\",\"ğŸ˜§\",\"ğŸ˜¨\",\"ğŸ˜©\",\"ğŸ˜ª\",\"ğŸ˜«\",\"ğŸ˜¬\",\"ğŸ˜­\",\"ğŸ˜¯\",\"ğŸ˜°\",\"ğŸ˜±\",\"ğŸ˜²\",\n",
        "               \"ğŸ˜³\",\"ğŸ˜´\",\"ğŸ˜·\",\"ğŸ˜¾\",\"ğŸ˜¿\",\"ğŸ™€\",\"ğŸ’€\",\"ğŸ‘\"]\n",
        "\n",
        "# Emails\n",
        "emailsRegex=re.compile(r'[\\w\\.-]+@[\\w\\.-]+')\n",
        "\n",
        "# Mentions\n",
        "userMentionsRegex=re.compile(r'(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9]+)')\n",
        "\n",
        "#Urls\n",
        "urlsRegex=re.compile('r(f|ht)(tp)(s?)(://)(.*)[.|/][^ ]+') # It may not be handling all the cases like t.co without http\n",
        "\n",
        "#Numerics\n",
        "numsRegex=re.compile(r\"\\b\\d+\\b\")\n",
        "\n",
        "punctuationNotEmoticonsRegex=re.compile(r'(?<=\\w)[^\\s\\w](?![^\\s\\w])')\n",
        "\n",
        "emoticonsDict = {}\n",
        "for i,each in enumerate(pos_emoticons):\n",
        "    emoticonsDict[each]=' POS_EMOTICON_'+num2words.num2words(i).upper()+' '\n",
        "    \n",
        "for i,each in enumerate(neg_emoticons):\n",
        "    emoticonsDict[each]=' NEG_EMOTICON_'+num2words.num2words(i).upper()+' '\n",
        "    \n",
        "# use these three lines to do the replacement\n",
        "rep = dict((re.escape(k), v) for k, v in emoticonsDict.items())\n",
        "emoticonsPattern = re.compile(\"|\".join(rep.keys()))\n",
        "\n",
        "\n",
        "def read_data(filename):   \n",
        "    \"\"\"Read the raw tweet data from a file. Replace Emails etc with special tokens\"\"\"\n",
        "    with open(filename, 'r') as f:\n",
        "    \n",
        "        all_lines=f.readlines()\n",
        "        padded_lines=[]\n",
        "        for line in all_lines:\n",
        "                    line = emoticonsPattern.sub(lambda m: rep[re.escape(m.group(0))], line.lower().strip())\n",
        "                    line = userMentionsRegex.sub(' USER ', line )\n",
        "                    line = emailsRegex.sub(' EMAIL ', line )\n",
        "                    line=urlsRegex.sub(' URL ', line)\n",
        "                    line=numsRegex.sub(' NUM ',line)\n",
        "                    line=punctuationNotEmoticonsRegex.sub(' PUN ',line)\n",
        "                    line=re.sub(r'(.)\\1{2,}', r'\\1\\1',line)\n",
        "                    words_tokens=[token for token in TweetTokenizer().tokenize(line)]                   \n",
        "                    line= ' '.join(token for token in words_tokens )        \n",
        "                    padded_lines.append(line)\n",
        "        return padded_lines\n",
        "    \n",
        "    \n",
        "def read_labels(filename):\n",
        "    \"\"\" read the tweet labels from the file\"\"\"\n",
        "    arr= np.genfromtxt(filename, delimiter='\\n')\n",
        "    arr[arr==4]=1 # Encode the positive category as 1\n",
        "    return arr\n",
        "\n",
        "# Loading Training and Validation Data\n",
        "texts \t\t\t\t= []\n",
        "labels \t\t\t\t= []\n",
        "nb_train_samples\t= 0\n",
        "nb_valid_samples \t= 0\n",
        "\n",
        "print ('Loading Training Labels')\n",
        "train_labels=read_labels(data_dir+'//training_label.csv')\n",
        "\n",
        "print ('Loading Training data')\n",
        "train_texts=read_data(data_dir+'//training_text.csv')\n",
        "\n",
        "print (len(train_labels), len(train_texts))\n",
        "print (\"Using Keras tokenizer to tokenize and build word index\")\n",
        "tokenizer = Tokenizer(lower=False, filters='\\n\\t?\"!') \n",
        "train_texts=[each for each in train_texts]\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "sorted_voc = [wc[0] for wc in sorted(tokenizer.word_counts.items(),reverse=True, key= lambda x:x[1]) ]\n",
        "tokenizer.word_index = dict(list(zip(sorted_voc, list(range(2, len(sorted_voc) + 2)))))\n",
        "tokenizer.word_index['<PAD>']=0\n",
        "tokenizer.word_index['<UNK>']=1\n",
        "word_index = tokenizer.word_index\n",
        "reverse_dictionary={v:k for (k,v) in tokenizer.word_index.items()}\n",
        "vocab_size=len(tokenizer.word_index.keys())\n",
        "\n",
        "print ('Size of the vocab is', vocab_size)\n",
        "\n",
        "\n",
        "\n",
        "print ('Padding sentences and shuffling the data')\n",
        "sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "\n",
        "#Pad the sentences to have consistent length\n",
        "data = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')\n",
        "labels = to_categorical(np.asarray(train_labels))\n",
        "indices = np.arange(len(labels))\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "train_x, valid_x, train_y, valid_y=train_test_split(data, labels, test_size=0.2, random_state=random_seed)\n",
        "train_x=np.array(train_x).astype('float32')\n",
        "valid_x=np.array(valid_x).astype('float32')\n",
        "train_y=np.array(train_y)\n",
        "valid_y=np.array(valid_y)\n",
        "embedding_matrix = np.zeros((len(word_index) , embedding_dim))\n",
        "training_word_index=tokenizer.word_index.copy()\n",
        "\n",
        "\n",
        "print ('Initializing the model')\n",
        "mcp = ModelCheckpoint('./model_chkpoint', monitor=\"val_acc\", save_best_only=True, save_weights_only=False)\n",
        "\n",
        "#Creating network\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(word_index)+2,\n",
        "                            embedding_dim,\n",
        "                            input_length=max_sequence_length,\n",
        "                            trainable=trainable, name='embedding'))\n",
        "model.add(Convolution1D(no_filters, filter_size, activation='relu'))\n",
        "model.add(MaxPooling1D(max_sequence_length - filter_size))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(no_filters, activation='tanh'))\n",
        "model.add(Dense(len(labels[0]), activation='softmax'))\n",
        "\n",
        "optim=optimizers.Adam(lr=0.1, )\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optim,\n",
        "              metrics=['acc'])\n",
        "model.summary()\n",
        "\n",
        "start=timer()\n",
        "hist=model.fit(train_x, train_y,nb_epoch=no_epochs, batch_size=batch_size,validation_data=(valid_x, valid_y),callbacks=[mcp])\n",
        "end=timer()\n",
        "\n",
        "# Exporting the Embedding Matrix and Vocabulary\n",
        "def export_embeddings(model_orig):\n",
        "    \"\"\" export embeddings to file\"\"\"\n",
        "    embedding_weights=pd.DataFrame(model_orig.layers[0].get_weights()[0]).reset_index()\n",
        "    word_indices_df=pd.DataFrame.from_dict(training_word_index,orient='index').reset_index()\n",
        "    word_indices_df.columns=['word','index']\n",
        "    print (word_indices_df.shape,embedding_weights.shape)\n",
        "    merged=pd.merge(word_indices_df,embedding_weights)\n",
        "    print (merged.shape)\n",
        "    merged=merged[[each for each in merged.columns if each!='index']]    \n",
        "    merged.to_csv(embedding_folder+'//embeddings_{}.tsv'.format(model_identifier), sep='\\t', \n",
        "              index=False, header=False,float_format='%.6f',encoding='utf-8')\n",
        "    return embedding_weights, word_indices_df, merged\n",
        "\n",
        "embedding_weights, word_indices_df, merged_df=export_embeddings(model)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Training Labels\n",
            "Loading Training data\n",
            "1280000 1280000\n",
            "Using Keras tokenizer to tokenize and build word index\n",
            "Size of the vocab is 304268\n",
            "Padding sentences and shuffling the data\n",
            "Initializing the model\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 15, 50)            15213500  \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 11, 15)            3765      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 1, 15)             0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 15)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 15)                240       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 32        \n",
            "=================================================================\n",
            "Total params: 15,217,537\n",
            "Trainable params: 15,217,537\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:190: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 1024000 samples, validate on 256000 samples\n",
            "Epoch 1/5\n",
            "1024000/1024000 [==============================] - 50s 49us/step - loss: 0.5045 - acc: 0.7554 - val_loss: 0.4763 - val_acc: 0.7753\n",
            "Epoch 2/5\n",
            "1024000/1024000 [==============================] - 50s 49us/step - loss: 0.4693 - acc: 0.7783 - val_loss: 0.4957 - val_acc: 0.7675\n",
            "Epoch 3/5\n",
            "1024000/1024000 [==============================] - 50s 49us/step - loss: 0.4796 - acc: 0.7706 - val_loss: 0.5000 - val_acc: 0.7594\n",
            "Epoch 4/5\n",
            "1024000/1024000 [==============================] - 50s 48us/step - loss: 0.5021 - acc: 0.7596 - val_loss: 0.5337 - val_acc: 0.7408\n",
            "Epoch 5/5\n",
            "1024000/1024000 [==============================] - 52s 51us/step - loss: 0.5277 - acc: 0.7427 - val_loss: 0.5556 - val_acc: 0.6829\n",
            "(304268, 2) (304270, 51)\n",
            "(304268, 52)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbWR0K_tFyWi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "07a7dbb5-c053-4226-d817-e5d72e51274d"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import io\n",
        "\n",
        "random_seed=1\n",
        "\n",
        "np.random.seed(random_seed)\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, merge\n",
        "from keras.layers.core import Lambda\n",
        "from keras import optimizers\n",
        "from keras import regularizers\n",
        "from keras.models import load_model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Input, Dense, Flatten, Embedding , Activation\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import re\n",
        "import num2words\n",
        "from timeit import default_timer as timer\n",
        "from sklearn import  metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.externals import joblib"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cEWUFCSGa8B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Path of the training file'\n",
        "data_dir = r'/content/data'\n",
        "\n",
        "# Path of the word vectors\n",
        "vectors_file = r'/content/data/vectors/embeddings_SSWE_Basic_Keras_w_CNTK.tsv' \n",
        "\n",
        "model_identifier='evaluation_SSWE_logistic'\n",
        "models_dir='model'\n",
        "if not os.path.exists(models_dir):\n",
        "    os.makedirs(models_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NO5fLUYAGsDp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "pos_emoticons=[\"(^.^)\",\"(^-^)\",\"(^_^)\",\"(^_~)\",\"(^3^)\",\"(^o^)\",\"(~_^)\",\"*)\",\":)\",\":*\",\":-*\",\":]\",\":^)\",\":}\",\n",
        "               \":>\",\":3\",\":b\",\":-b\",\":c)\",\":D\",\":-D\",\":O\",\":-O\",\":o)\",\":p\",\":-p\",\":P\",\":-P\",\":Ã\",\":-Ã\",\":X\",\n",
        "               \":-X\",\";)\",\";-)\",\";]\",\";D\",\"^)\",\"^.~\",\"_)m\",\" ~.^\",\"<=8\",\"<3\",\"<333\",\"=)\",\"=///=\",\"=]\",\"=^_^=\",\n",
        "               \"=<_<=\",\"=>.<=\",\" =>.>=\",\" =3\",\"=D\",\"=p\",\"0-0\",\"0w0\",\"8D\",\"8O\",\"B)\",\"C:\",\"d'-'\",\"d(>w<)b\",\":-)\",\n",
        "               \"d^_^b\",\"qB-)\",\"X3\",\"xD\",\"XD\",\"XP\",\"Ê˜â€¿Ê˜\",\"â¤\",\"ğŸ’œ\",\"ğŸ’š\",\"ğŸ’•\",\"ğŸ’™\",\"ğŸ’›\",\"ğŸ’“\",\"ğŸ’\",\"ğŸ’–\",\"ğŸ’\",\n",
        "               \"ğŸ’˜\",\"ğŸ’—\",\"ğŸ˜—\",\"ğŸ˜˜\",\"ğŸ˜™\",\"ğŸ˜š\",\"ğŸ˜»\",\"ğŸ˜€\",\"ğŸ˜\",\"ğŸ˜ƒ\",\"â˜º\",\"ğŸ˜„\",\"ğŸ˜†\",\"ğŸ˜‡\",\"ğŸ˜‰\",\"ğŸ˜Š\",\"ğŸ˜‹\",\"ğŸ˜\",\n",
        "               \"ğŸ˜\",\"ğŸ˜\",\"ğŸ˜›\",\"ğŸ˜œ\",\"ğŸ˜\",\"ğŸ˜®\",\"ğŸ˜¸\",\"ğŸ˜¹\",\"ğŸ˜º\",\"ğŸ˜»\",\"ğŸ˜¼\",\"ğŸ‘\"]\n",
        "\n",
        "neg_emoticons=[\"--!--\",\"(,_,)\",\"(-.-)\",\"(._.)\",\"(;.;)9\",\"(>.<)\",\"(>_<)\",\"(>_>)\",\"(Â¬_Â¬)\",\"(X_X)\",\":&\",\":(\",\":'(\",\n",
        "               \":-(\",\":-/\",\":-@[1]\",\":[\",\":\\\\\",\":{\",\":<\",\":-9\",\":c\",\":S\",\";(\",\";*(\",\";_;\",\"^>_>^\",\"^o)\",\"_|_\",\n",
        "               \"`_Â´\",\"</3\",\"<=3\",\"=/\",\"=\\\\\",\">:(\",\">:-(\",\"ğŸ’”\",\"â˜¹ï¸\",\"ğŸ˜Œ\",\"ğŸ˜’\",\"ğŸ˜“\",\"ğŸ˜”\",\"ğŸ˜•\",\"ğŸ˜–\",\"ğŸ˜\",\"ğŸ˜Ÿ\",\n",
        "               \"ğŸ˜ \",\"ğŸ˜¡\",\"ğŸ˜¢\",\"ğŸ˜£\",\"ğŸ˜¤\",\"ğŸ˜¥\",\"ğŸ˜¦\",\"ğŸ˜§\",\"ğŸ˜¨\",\"ğŸ˜©\",\"ğŸ˜ª\",\"ğŸ˜«\",\"ğŸ˜¬\",\"ğŸ˜­\",\"ğŸ˜¯\",\"ğŸ˜°\",\"ğŸ˜±\",\"ğŸ˜²\",\n",
        "               \"ğŸ˜³\",\"ğŸ˜´\",\"ğŸ˜·\",\"ğŸ˜¾\",\"ğŸ˜¿\",\"ğŸ™€\",\"ğŸ’€\",\"ğŸ‘\"]\n",
        "\n",
        "# Emails\n",
        "emailsRegex=re.compile(r'[\\w\\.-]+@[\\w\\.-]+')\n",
        "\n",
        "# Mentions\n",
        "userMentionsRegex=re.compile(r'(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9]+)')\n",
        "\n",
        "#Urls\n",
        "urlsRegex=re.compile('r(f|ht)(tp)(s?)(://)(.*)[.|/][^ ]+') # It may not be handling all the cases like t.co without http\n",
        "\n",
        "#Numerics\n",
        "numsRegex=re.compile(r\"\\b\\d+\\b\")\n",
        "\n",
        "punctuationNotEmoticonsRegex=re.compile(r'(?<=\\w)[^\\s\\w](?![^\\s\\w])')\n",
        "\n",
        "emoticonsDict = {}\n",
        "for i,each in enumerate(pos_emoticons):\n",
        "    emoticonsDict[each]=' POS_EMOTICON_'+num2words.num2words(i).upper()+' '\n",
        "    \n",
        "for i,each in enumerate(neg_emoticons):\n",
        "    emoticonsDict[each]=' NEG_EMOTICON_'+num2words.num2words(i).upper()+' '\n",
        "    \n",
        "# use these three lines to do the replacement\n",
        "rep = dict((re.escape(k), v) for k, v in emoticonsDict.items())\n",
        "emoticonsPattern = re.compile(\"|\".join(rep.keys()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyFpADBbGto7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_data(filename):\n",
        "    \"\"\"Read the raw tweet data from a file. Replace Emails etc with special tokens\"\"\"\n",
        "    with open(filename, 'r') as f:\n",
        "    \n",
        "        all_lines=f.readlines()\n",
        "        padded_lines=[]\n",
        "        for line in all_lines:\n",
        "                    line = emoticonsPattern.sub(lambda m: rep[re.escape(m.group(0))], line.lower().strip())\n",
        "                    line = userMentionsRegex.sub(' USER ', line )\n",
        "                    line = emailsRegex.sub(' EMAIL ', line )\n",
        "                    line=urlsRegex.sub(' URL ', line)\n",
        "                    line=numsRegex.sub(' NUM ',line)\n",
        "                    line=punctuationNotEmoticonsRegex.sub(' PUN ',line)\n",
        "                    line=re.sub(r'(.)\\1{2,}', r'\\1\\1',line)\n",
        "                    words_tokens=[token for token in TweetTokenizer().tokenize(line)]                   \n",
        "                    line= ' '.join(token for token in words_tokens )        \n",
        "                    padded_lines.append(line)\n",
        "        return padded_lines\n",
        "    \n",
        "def read_labels(filename):\n",
        "    \"\"\" read the tweet labels from the file\"\"\"\n",
        "    arr= np.genfromtxt(filename, delimiter='\\n')\n",
        "    arr[arr==4]=1 # Encode the positive category as 1\n",
        "    return arr\n",
        "\n",
        "# Convert Word Vectors to Sentence Vectors\n",
        "\n",
        "def load_word_embedding(vectors_file):\n",
        "    \"\"\" Load the word vectors\"\"\"\n",
        "    vectors= np.genfromtxt(vectors_file, delimiter='\\t', comments='#--#',dtype=None,\n",
        "                           names=['Word']+['EV{}'.format(i) for i in range(1,51)])\n",
        "    # comments have to be changed as some of the tokens are having # in them and then we dont need comments\n",
        "    vectors_dc={}\n",
        "    for x in vectors:\n",
        "        vectors_dc[x['Word'].decode('utf-8','ignore')]=[float(x[each]) for each in ['EV{}'.format(i) for i in range(1,51)]]\n",
        "    return vectors_dc\n",
        "\n",
        "def get_sentence_embedding(text_data, vectors_dc):\n",
        "    sentence_vectors=[]\n",
        "    \n",
        "    for sen in text_data:\n",
        "        tokens=sen.split(' ')\n",
        "        current_vector=np.array([vectors_dc[tokens[0]] if tokens[0] in vectors_dc else vectors_dc['<UNK>']])\n",
        "        for word in tokens[1:]:\n",
        "            if word in vectors_dc:\n",
        "                current_vector=np.vstack([current_vector,vectors_dc[word]])\n",
        "            else:\n",
        "                current_vector=np.vstack([current_vector,vectors_dc['<UNK>']])\n",
        "        min_max_mean=np.hstack([current_vector.min(axis=0),current_vector.max(axis=0),current_vector.mean(axis=0)])\n",
        "        sentence_vectors.append(min_max_mean)\n",
        "    return sentence_vectors\n",
        "\n",
        "\n",
        "# Model Training\n",
        "\n",
        "batch_size = 1028*6 # Batch Size should be changed according to the system specifications to have better utilization of GPU\n",
        "nb_epoch = 10\n",
        "\n",
        "def init_model():\n",
        "    output_dim = no_classes = len(to_categorical(train_y)[0])\n",
        "    input_dim=150\n",
        "    model = Sequential() \n",
        "    model.add(Dense(output_dim, input_dim=input_dim, activation='softmax',activity_regularizer=regularizers.l1_l2(1))) \n",
        "    return model\n",
        "\n",
        "def cv_estimate(n_splits,X_train, y_train):\n",
        "    best_score, best_model= None,None\n",
        "    cv = KFold(n_splits=n_splits)\n",
        "    \n",
        "    i=0\n",
        "    for train, test in cv.split(X_train, y_train):\n",
        "        model=init_model()\n",
        "        mcp = ModelCheckpoint('./model_chkpoint_{}'.format(i), monitor=\"val_acc\",\n",
        "                      save_best_only=True, save_weights_only=False)\n",
        "        model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']) \n",
        "\n",
        "        model.fit(X_train[train], to_categorical(y_train[train]),epochs=nb_epoch,batch_size=batch_size, callbacks=[mcp],\n",
        "                  validation_split=0.2)\n",
        "        current_score= model.evaluate(X_train[test], to_categorical(y_train[test]))[0] # Getting the loss\n",
        "        print ('\\n Fold {} Current score {}'.format(i+1, current_score))\n",
        "                \n",
        "        if i==0:\n",
        "            best_score=current_score\n",
        "            best_model=model\n",
        "        else:\n",
        "            print (current_score)\n",
        "            if current_score<best_score:\n",
        "                best_score=current_score\n",
        "                best_model=model\n",
        "        i+=1\n",
        "\n",
        "    return  best_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZNqSiWXG12M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print ('Step 1: Loading Training data')\n",
        "train_texts=read_data(data_dir+'/training_text.csv')\n",
        "print ('Reading training labels')\n",
        "train_labels=read_labels(data_dir+'/training_label.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9oRKv9xIilB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6f962d79-fb72-4c1b-bf31-33924baabc69"
      },
      "source": [
        "\n",
        "print (\"Step 2: Load Word Vectors\")\n",
        "vectors_dc=load_word_embedding(vectors_file)\n",
        "len(vectors_dc)\n",
        "\n",
        "print (\"Step 3: Converting the word vectors to sentence vectors\")\n",
        "train_sentence_vectors=get_sentence_embedding(train_texts,vectors_dc)\n",
        "print (len(train_sentence_vectors), len(train_labels), len(train_texts))\n",
        "print (\" Encoding the input\")\n",
        "train_x=train_sentence_vectors\n",
        "train_y=train_labels\n",
        "train_x=np.array(train_x).astype('float32')\n",
        "train_y=np.array(train_y)\n",
        "\n",
        "print ('Step 4: Logistic regression model using Keras')\n",
        "best_model=cv_estimate(3,train_x, train_y)\n",
        "\n",
        "print (\"Step 5: Saving the model\")\n",
        "best_model.save(models_dir+'//'+model_identifier)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 2: Load Word Vectors\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step 3: Converting the word vectors to sentence vectors\n",
            "1280000 1280000 1280000\n",
            " Encoding the input\n",
            "Step 4: Logistic regression model using Keras\n",
            "Train on 682666 samples, validate on 170667 samples\n",
            "Epoch 1/10\n",
            "682666/682666 [==============================] - 1s 2us/step - loss: 6218.9968 - accuracy: 0.5622 - val_loss: 6174.2761 - val_accuracy: 0.5388\n",
            "Epoch 2/10\n",
            "104856/682666 [===>..........................] - ETA: 1s - loss: 6233.2565 - accuracy: 0.5629"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
            "  'skipping.' % (self.monitor), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "682666/682666 [==============================] - 1s 2us/step - loss: 6220.5022 - accuracy: 0.5644 - val_loss: 6176.7012 - val_accuracy: 0.6383\n",
            "Epoch 3/10\n",
            "682666/682666 [==============================] - 1s 2us/step - loss: 6218.8402 - accuracy: 0.5737 - val_loss: 6175.7766 - val_accuracy: 0.6556\n",
            "Epoch 4/10\n",
            "682666/682666 [==============================] - 1s 2us/step - loss: 6219.1211 - accuracy: 0.5687 - val_loss: 6173.3127 - val_accuracy: 0.6631\n",
            "Epoch 5/10\n",
            "682666/682666 [==============================] - 1s 2us/step - loss: 6220.0692 - accuracy: 0.5683 - val_loss: 6177.1511 - val_accuracy: 0.5311\n",
            "Epoch 6/10\n",
            "682666/682666 [==============================] - 1s 2us/step - loss: 6217.8137 - accuracy: 0.5840 - val_loss: 6172.1788 - val_accuracy: 0.6094\n",
            "Epoch 7/10\n",
            "682666/682666 [==============================] - 1s 2us/step - loss: 6220.7768 - accuracy: 0.5651 - val_loss: 6177.7819 - val_accuracy: 0.5554\n",
            "Epoch 8/10\n",
            "682666/682666 [==============================] - 1s 2us/step - loss: 6216.3187 - accuracy: 0.5979 - val_loss: 6174.0900 - val_accuracy: 0.6944\n",
            "Epoch 9/10\n",
            "682666/682666 [==============================] - 1s 2us/step - loss: 6217.7209 - accuracy: 0.5759 - val_loss: 6185.2636 - val_accuracy: 0.5017\n",
            "Epoch 10/10\n",
            "682666/682666 [==============================] - 1s 2us/step - loss: 6217.9645 - accuracy: 0.5710 - val_loss: 6173.9085 - val_accuracy: 0.5010\n",
            "426667/426667 [==============================] - 6s 15us/step\n",
            "\n",
            " Fold 1 Current score 33.970163093254165\n",
            "Train on 682666 samples, validate on 170667 samples\n",
            "Epoch 1/10\n",
            "682666/682666 [==============================] - 1s 2us/step - loss: 6220.6875 - accuracy: 0.5608 - val_loss: 6173.7266 - val_accuracy: 0.6389\n",
            "Epoch 2/10\n",
            "682666/682666 [==============================] - 1s 2us/step - loss: 6219.8697 - accuracy: 0.5598 - val_loss: 6178.1992 - val_accuracy: 0.6483\n",
            "Epoch 3/10\n",
            "682666/682666 [==============================] - 1s 2us/step - loss: 6218.0314 - accuracy: 0.5807 - val_loss: 6185.8348 - val_accuracy: 0.4991\n",
            "Epoch 4/10\n",
            "682666/682666 [==============================] - 1s 2us/step - loss: 6220.4588 - accuracy: 0.5613 - val_loss: 6190.6175 - val_accuracy: 0.4990\n",
            "Epoch 5/10\n",
            "682666/682666 [==============================] - 1s 2us/step - loss: 6220.1960 - accuracy: 0.5735 - val_loss: 6184.5109 - val_accuracy: 0.4997\n",
            "Epoch 6/10\n",
            "682666/682666 [==============================] - 1s 2us/step - loss: 6218.6981 - accuracy: 0.5772 - val_loss: 6174.3644 - val_accuracy: 0.6950\n",
            "Epoch 7/10\n",
            "682666/682666 [==============================] - 1s 2us/step - loss: 6218.5993 - accuracy: 0.5696 - val_loss: 6180.6727 - val_accuracy: 0.5396\n",
            "Epoch 8/10\n",
            "682666/682666 [==============================] - 1s 2us/step - loss: 6217.9539 - accuracy: 0.5746 - val_loss: 6185.9649 - val_accuracy: 0.5229\n",
            "Epoch 9/10\n",
            "682666/682666 [==============================] - 1s 2us/step - loss: 6218.8183 - accuracy: 0.5775 - val_loss: 6177.2142 - val_accuracy: 0.6911\n",
            "Epoch 10/10\n",
            "682666/682666 [==============================] - 1s 2us/step - loss: 6219.0741 - accuracy: 0.5843 - val_loss: 6182.2623 - val_accuracy: 0.5549\n",
            "426667/426667 [==============================] - 6s 15us/step\n",
            "\n",
            " Fold 2 Current score 36.55251464353532\n",
            "36.55251464353532\n",
            "Train on 682667 samples, validate on 170667 samples\n",
            "Epoch 1/10\n",
            "682667/682667 [==============================] - 1s 2us/step - loss: 6217.4479 - accuracy: 0.5751 - val_loss: 6174.7141 - val_accuracy: 0.6420\n",
            "Epoch 2/10\n",
            "682667/682667 [==============================] - 1s 2us/step - loss: 6219.4411 - accuracy: 0.5596 - val_loss: 6187.2110 - val_accuracy: 0.5002\n",
            "Epoch 3/10\n",
            "682667/682667 [==============================] - 1s 2us/step - loss: 6224.8068 - accuracy: 0.5366 - val_loss: 6178.4770 - val_accuracy: 0.5466\n",
            "Epoch 4/10\n",
            "682667/682667 [==============================] - 1s 2us/step - loss: 6218.0950 - accuracy: 0.5732 - val_loss: 6175.3353 - val_accuracy: 0.6661\n",
            "Epoch 5/10\n",
            "682667/682667 [==============================] - 1s 2us/step - loss: 6220.3528 - accuracy: 0.5517 - val_loss: 6193.0542 - val_accuracy: 0.5006\n",
            "Epoch 6/10\n",
            "682667/682667 [==============================] - 1s 2us/step - loss: 6221.7507 - accuracy: 0.5591 - val_loss: 6181.0094 - val_accuracy: 0.5032\n",
            "Epoch 7/10\n",
            "682667/682667 [==============================] - 1s 2us/step - loss: 6218.1098 - accuracy: 0.5788 - val_loss: 6174.5913 - val_accuracy: 0.5909\n",
            "Epoch 8/10\n",
            "682667/682667 [==============================] - 1s 2us/step - loss: 6221.6290 - accuracy: 0.5683 - val_loss: 6184.9608 - val_accuracy: 0.5413\n",
            "Epoch 9/10\n",
            "682667/682667 [==============================] - 1s 2us/step - loss: 6220.8212 - accuracy: 0.5586 - val_loss: 6191.1356 - val_accuracy: 0.5002\n",
            "Epoch 10/10\n",
            "682667/682667 [==============================] - 1s 2us/step - loss: 6216.9578 - accuracy: 0.5862 - val_loss: 6183.3443 - val_accuracy: 0.5093\n",
            "426666/426666 [==============================] - 6s 15us/step\n",
            "\n",
            " Fold 3 Current score 36.90495058503071\n",
            "36.90495058503071\n",
            "Step 5: Saving the model\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}